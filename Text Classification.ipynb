{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "Text classification is the process of assigning tags or categories to text according to its content. It’s one of the fundamental tasks in natural language processing. \n",
    "\n",
    "The text we wanna classify is given as input to an algorithm, the algorithm will then analyze the text’s content, and then categorize the input as one of the tags or categories previously given.\n",
    "\n",
    "**Input → Classifying Algorithm → Classification of Input**\n",
    "\n",
    "Real life examples: \n",
    "\n",
    "+ sentiment analysis: how does the writer of the sentence feel about what they are writing about, do they think positively or negatively of the subject?\n",
    "Ex. restaurant reviews\n",
    "topic labeling: given sentences and a set of topics, which topic does this sentence fall under? \n",
    "Ex. is this essay about history? Math? etc?\n",
    "spam detection\n",
    "Ex. Email filtering: is this email a real important email or spam?\n",
    "\n",
    "Example. \n",
    "A restaurant wants to evaluate their ratings but don’t want to read through all of them. Therefore, they wanna use a computer algorithm to do all their work. They simply want to know if the customer’s review is positive or negative.\n",
    "\n",
    "Here’s an example of a customer’s review and a simple way an algorithm could classify their review.\n",
    "\n",
    "Input: “The food here was too salty and too expensive” \n",
    "\n",
    "Algorithm: \n",
    "Goes through every word in the sentence and counts how many positive words and how many negative words are in the sentence.\n",
    "\n",
    "\t\t“The, food, here, was, too, and” are all neutral words\n",
    "\n",
    "\t\t“Salty, expensive” are negative words.\n",
    "\n",
    "\t\tNegative words: 2\n",
    "\t\tPositive words: 0\n",
    "\n",
    "Classification: Negative Review, because there are more negative words (2) than positive (0).\n",
    "\n",
    "However, this algorithm obviously doesn’t work in a lot of cases. \n",
    "\n",
    "For example, “The food here was good, not expensive and not salty” would be classified as negative but it’s actually a positive review. \n",
    "\n",
    "Language and text can get very complicated which makes creating these algorithms difficult. Some things that make language difficult could be words that have multiple meanings, negation words (words such as not), slang, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import string\n",
    "from scipy import sparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingFile = \"trainingSet.txt\"\n",
    "testingFile = \"testSet.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(fileName):\n",
    "    f = open(fileName)\n",
    "    file = f.readlines()\n",
    "\n",
    "    sentences = []\n",
    "    sentiments = []\n",
    "\n",
    "    for line in file:\n",
    "        sentence, sentiment = line.split('\\t')\n",
    "        sentences.append(sentence.strip())\n",
    "        sentiments.append(int(sentiment.strip())) # Sentiment in {0,1}\n",
    "\n",
    "    return sentences, np.array(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSentences, trainingLabels = getData(trainingFile)\n",
    "testingSentences, testingLabels = getData(testingFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(sentences):\n",
    "\n",
    "    def cleanText(text):\n",
    "        # Make lower case\n",
    "        text = text.lower()\n",
    "\n",
    "        # Replace non-text characters with spaces\n",
    "        nonText = string.punctuation + (\"\")\n",
    "        text = text.translate(str.maketrans(nonText, ' ' * (len(nonText))))\n",
    "\n",
    "        # Tokenize\n",
    "        words = text.split()\n",
    "\n",
    "        return words\n",
    "\n",
    "    return list(map(cleanText, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingTokens = preProcess(trainingSentences)\n",
    "testingTokens = preProcess(testingSentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data and Setting it Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVocab(sentences):\n",
    "    vocab = set()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            vocab.add(word)\n",
    "    return sorted(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = getVocab(trainingTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7141bc2f4b84b2daf37332f22af3d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Click to see Vocab', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clicked(arg):\n",
    "    print(vocabulary)\n",
    "\n",
    "button_download = widgets.Button(description = 'Click to see Vocab')   \n",
    "button_download.on_click(clicked)\n",
    "display(button_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVector(vocab, sentences):\n",
    "    indices = []\n",
    "    wordOccurrences = []\n",
    "\n",
    "    for sentenceIndex, sentence in enumerate(sentences):\n",
    "        alreadyCounted = set() # Keep track of words so we don't double count.\n",
    "        for word in sentence:\n",
    "            if (word in vocab) and word not in alreadyCounted:\n",
    "                # If we just want {0,1} for the presence of the word (bernoulli NB),\n",
    "                # only count each word once. Otherwise (multinomial NB) count each\n",
    "                # occurrence of the word.\n",
    "                \n",
    "            \n",
    "                #which sentence, which word\n",
    "                indices.append((sentenceIndex, vocab.index(word)))\n",
    "                \n",
    "                wordOccurrences.append(1)\n",
    "                alreadyCounted.add(word)\n",
    "\n",
    "    # Unzip\n",
    "    rows = [row for row, _ in indices]\n",
    "    columns = [column for _, column in indices]\n",
    "\n",
    "    sentenceVectors = sparse.csr_matrix((wordOccurrences, (rows, columns)), dtype=int, shape=(len(sentences), len(vocab)))\n",
    "\n",
    "    return sentenceVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = createVector(vocabulary, trainingTokens)\n",
    "testing = createVector(vocabulary, testingTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.priorPositive = None  # Probability that a review is positive\n",
    "        self.priorNegative = None  # Probability that a review is negative\n",
    "        self.positiveLogConditionals = None\n",
    "        self.negativeLogConditionals = None\n",
    "\n",
    "    def computePriorProbabilities(self, labels):\n",
    "        self.priorPositive = len([y for y in labels if y == 1]) / len(labels)\n",
    "        self.priorNegative = 1 - self.priorPositive\n",
    "\n",
    "    def computeConditionProbabilities(self, examples, labels, dirichlet=1):\n",
    "        _, vocabularyLength = examples.shape\n",
    "\n",
    "        # How many of each word are there in all of the positive reviews\n",
    "        positiveCounts = np.array([dirichlet for _ in range(vocabularyLength)])\n",
    "        # How many of each word are there in all of the negative reviews\n",
    "        negativeCounts = np.array([dirichlet for _ in range(vocabularyLength)])\n",
    "\n",
    "        # Here's how to iterate through a spare array\n",
    "        coordinates = examples.tocoo()  # Converted to a `coordinate` format\n",
    "        for exampleIndex, featureIndex, observationCount in zip(coordinates.row, coordinates.col, coordinates.data):\n",
    "            # For sentence {exampleIndex}, for word at index {featureIndex}, the word occurred {observationCount} times\n",
    "            if labels[exampleIndex] == 1:\n",
    "                positiveCounts[featureIndex] += observationCount\n",
    "            else:\n",
    "                negativeCounts[featureIndex] += observationCount\n",
    "\n",
    "        # [!] Make sure to use the logs of the probabilities\n",
    "        positiveReviewCount = len([y for y in labels if y == 1])\n",
    "        negativeReviewCount = len([y for y in labels if y == 0])\n",
    "\n",
    "        # We are using bernoulli NB\n",
    "        self.positiveLogConditionals = np.log(positiveCounts) - np.log(positiveReviewCount + dirichlet*2)\n",
    "        self.negativeLogConditionals = np.log(negativeCounts) - np.log(negativeReviewCount + dirichlet*2)\n",
    "\n",
    "        # This works for multinomial NB\n",
    "        # self.positiveLogConditionals = np.log(positiveCounts) - np.log(sum(positiveCounts))\n",
    "        # self.negativeLogConditionals = np.log(negativeCounts) - np.log(sum(negativeCounts))\n",
    "\n",
    "    # Calculate all of the parameters for making a naive bayes classification\n",
    "    def fit(self, trainingExamples, trainingLabels):\n",
    "        # Compute the probability of positive/negative review\n",
    "        self.computePriorProbabilities(trainingLabels)\n",
    "\n",
    "        # Compute\n",
    "        self.computeConditionProbabilities(trainingExamples, trainingLabels)\n",
    "\n",
    "    def computeLogPosteriors(self, sentence):\n",
    "        return ((np.log(self.priorPositive) + sum(sentence * self.positiveLogConditionals)),\n",
    "                (np.log(self.priorNegative) + sum(sentence * self.negativeLogConditionals)))\n",
    "\n",
    "    def predict(self, examples):\n",
    "        totalReviewCount, _ = examples.shape\n",
    "\n",
    "        predictions = np.array([0 for _ in range(totalReviewCount)])\n",
    "\n",
    "        for index, sentence in enumerate(examples):\n",
    "            logProbabilityPositive, logProbabilityNegative = self.computeLogPosteriors(\n",
    "                sentence)\n",
    "            predictions[index] = 1 if logProbabilityPositive > logProbabilityNegative else 0\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbClassifier = NaiveBayesClassifier()\n",
    "nbClassifier.fit(training, trainingLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, actual):\n",
    "    return sum((predictions == actual)) / len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingPredictions = nbClassifier.predict(training)\n",
    "testingPredictions = nbClassifier.predict(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec603a2a46d48bfbb6570ddefcc6477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Model Accuracy', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9519038076152304\n",
      "Testing accuracy: 0.7947686116700201\n"
     ]
    }
   ],
   "source": [
    "def accuracy_button(arg):\n",
    "    print(\"Training accuracy:\", accuracy(trainingPredictions, trainingLabels))\n",
    "    print(\"Testing accuracy:\", accuracy(testingPredictions, testingLabels))\n",
    "\n",
    "button_download = widgets.Button(description = 'Model Accuracy')   \n",
    "button_download.on_click(accuracy_button)\n",
    "display(button_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "55400c05e9674a448f4e1fca689799eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ButtonStyleModel",
      "state": {}
     },
     "6d2c241530244303bf40c0e3b075e4ce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "91e9ccd185a845428527d57ccb73fb26": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "eec603a2a46d48bfbb6570ddefcc6477": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Model Accuracy",
       "layout": "IPY_MODEL_91e9ccd185a845428527d57ccb73fb26",
       "style": "IPY_MODEL_f8efef0d43ab4f32b7ab103f009ffe3b"
      }
     },
     "f7141bc2f4b84b2daf37332f22af3d8f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Click to see Vocab",
       "layout": "IPY_MODEL_6d2c241530244303bf40c0e3b075e4ce",
       "style": "IPY_MODEL_55400c05e9674a448f4e1fca689799eb"
      }
     },
     "f8efef0d43ab4f32b7ab103f009ffe3b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ButtonStyleModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
